# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-050-time_quadratic.txt
# order     : 1
# steps     : 50
# skip_type : time_quadratic
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99969620    1
# 0.99930996    3
# 0.99862993    7
# 0.99751961   10
# 0.99580753   15
# 0.99329579   20
# 0.98974973   26
# 0.98490953   33
# 0.97849500   41
# 0.97020745   49
# 0.95973396   58
# 0.94675821   68
# 0.93097502   79
# 0.91209573   90
# 0.88986635  102
# 0.86408037  115
# 0.83459920  129
# 0.80136335  143
# 0.76441383  158
# 0.72390133  174
# 0.68009883  191
# 0.63340271  208
# 0.58433783  226
# 0.53354198  245
# 0.48175344  265
# 0.42977899  285
# 0.37847048  306
# 0.32867709  328
# 0.28120837  350
# 0.23678841  374
# 0.19602349  398
# 0.15936618  423
# 0.12709631  448
# 0.09931426  475
# 0.07594710  502
# 0.05676719  530
# 0.04142054  558
# 0.02946457  588
# 0.02040645  618
# 0.01374107  649
# 0.00898346  680
# 0.00569389  713
# 0.00349358  746
# 0.00207191  780
# 0.00118584  814
# 0.00065396  850
# 0.00034693  886
# 0.00017675  923
# 0.00008633  960
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:553.868001 = 553.858845 + 0.009155
# loss_var     : 645.304503 => 553.858845
# model.learning_portion: 0.01
# model.out_channels    : 50
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999967:   0: 0.999967; 0.000033*1825.159385= 0.060274;  0.060274/0.999967=  0.060276
0.999855:   0: 0.999888; 0.000040*1624.542519= 0.064516;  0.064516/0.999855=  0.064525
0.999640:   1: 0.999785; 0.000048*1357.770627= 0.065004;  0.065004/0.999640=  0.065027
0.999312:   3: 0.999672; 0.000053*1185.035964= 0.062555;  0.062555/0.999312=  0.062598
0.998853:   6: 0.999541; 0.000058*1049.719117= 0.061308;  0.061308/0.998853=  0.061378
0.998241:   8: 0.999387; 0.000065* 941.450369= 0.061570;  0.061570/0.998241=  0.061678
0.997458:  11: 0.999216; 0.000072* 850.514980= 0.061330;  0.061330/0.997458=  0.061486
0.996483:  13: 0.999022; 0.000079* 776.009278= 0.061629;  0.061629/0.996483=  0.061846
0.995301:  16: 0.998814; 0.000086* 712.007258= 0.061298;  0.061298/0.995301=  0.061588
0.993894:  19: 0.998586; 0.000093* 657.645874= 0.061152;  0.061152/0.993894=  0.061528
0.992262:  22: 0.998358; 0.000098* 611.009280= 0.059782;  0.059782/0.992262=  0.060248
0.988699:  28: 0.996410; 0.000342* 540.498538= 0.184877;  0.184877/0.988699=  0.186990
0.982066:  37: 0.993291; 0.000782* 462.410884= 0.361740;  0.361740/0.982066=  0.368346
0.971942:  48: 0.989691; 0.001175* 394.597520= 0.463695;  0.463695/0.971942=  0.477081
0.957943:  60: 0.985597; 0.001504* 339.209401= 0.510223;  0.510223/0.957943=  0.532624
0.939733:  73: 0.980990; 0.001796* 294.896363= 0.529524;  0.529524/0.939733=  0.563484
0.917040:  87: 0.975852; 0.002072* 258.007342= 0.534512;  0.534512/0.917040=  0.582866
0.889665: 102: 0.970149; 0.002349* 227.894939= 0.535411;  0.535411/0.889665=  0.601812
0.857517: 118: 0.963865; 0.002638* 202.388637= 0.533841;  0.533841/0.857517=  0.622542
0.820624: 135: 0.956976; 0.002945* 180.580110= 0.531827;  0.531827/0.820624=  0.648077
0.779152: 152: 0.949463; 0.003278* 161.584551= 0.529743;  0.529743/0.779152=  0.679898
0.733426: 170: 0.941313; 0.003643* 145.277720= 0.529317;  0.529317/0.733426=  0.721704
0.683928: 189: 0.932511; 0.004048* 130.301634= 0.527428;  0.527428/0.683928=  0.771175
0.631297: 209: 0.923046; 0.004499* 117.047433= 0.526541;  0.526541/0.631297=  0.834063
0.576316: 229: 0.912908; 0.005005* 105.163311= 0.526317;  0.526317/0.576316=  0.913244
0.519889: 250: 0.902090; 0.005577*  94.475069= 0.526844;  0.526844/0.519889=  1.013377
0.463008: 272: 0.890589; 0.006225*  84.463362= 0.525806;  0.525806/0.463008=  1.135631
0.406712: 294: 0.878414; 0.006963*  75.231953= 0.523864;  0.523864/0.406712=  1.288045
0.352032: 317: 0.865554; 0.007807*  66.948078= 0.522697;  0.522697/0.352032=  1.484802
0.299937: 341: 0.852016; 0.008775*  59.192415= 0.519439;  0.519439/0.299937=  1.731828
0.251293: 366: 0.837821; 0.009886*  51.935864= 0.513425;  0.513425/0.251293=  2.043130
0.206808: 391: 0.822975; 0.011162*  45.114152= 0.503563;  0.503563/0.206808=  2.434928
0.166996: 417: 0.807492; 0.012629*  38.903620= 0.491330;  0.491330/0.166996=  2.942166
0.132159: 444: 0.791392; 0.014316*  33.188581= 0.475118;  0.475118/0.132159=  3.595042
0.102383: 472: 0.774697; 0.016251*  27.836520= 0.452366;  0.452366/0.102383=  4.418356
0.077549: 500: 0.757440; 0.018466*  23.028478= 0.425233;  0.425233/0.077549=  5.483390
0.057359: 529: 0.739642; 0.020994*  18.697286= 0.392531;  0.392531/0.057359=  6.843437
0.041375: 558: 0.721339; 0.023869*  14.913019= 0.355953;  0.355953/0.041375=  8.603093
0.029069: 589: 0.702563; 0.027123*  11.549654= 0.313265;  0.313265/0.029069= 10.776760
0.019864: 620: 0.683357; 0.030789*   8.746727= 0.269301;  0.269301/0.019864= 13.557121
0.013185: 652: 0.663756; 0.034896*   6.416124= 0.223899;  0.223899/0.013185= 16.981371
0.008489: 684: 0.643810; 0.039472*   4.556581= 0.179859;  0.179859/0.008489= 21.188188
0.005293: 718: 0.623558; 0.044543*   3.103129= 0.138222;  0.138222/0.005293= 26.113341
0.003192: 752: 0.603054; 0.050129*   2.024736= 0.101498;  0.101498/0.003192= 31.797021
0.001859: 786: 0.582334; 0.056255*   1.269379= 0.071409;  0.071409/0.001859= 38.415698
0.001044: 822: 0.561468; 0.062932*   0.754034= 0.047453;  0.047453/0.001044= 45.467154
0.000564: 858: 0.540500; 0.070180*   0.431834= 0.030306;  0.030306/0.000564= 53.723447
0.000293: 895: 0.519464; 0.078019*   0.238346= 0.018595;  0.018595/0.000293= 63.458273
0.000146: 933: 0.498425; 0.086458*   0.131335= 0.011355;  0.011355/0.000146= 77.744390
0.000070: 971: 0.477505; 0.095480*   0.074821= 0.007144;  0.007144/0.000070=102.432772
