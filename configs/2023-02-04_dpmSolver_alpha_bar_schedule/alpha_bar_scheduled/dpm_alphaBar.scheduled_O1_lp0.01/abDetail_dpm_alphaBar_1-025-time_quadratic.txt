# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-025-time_quadratic.txt
# order     : 1
# steps     : 25
# skip_type : time_quadratic
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99930996    3
# 0.99751961   10
# 0.99329579   20
# 0.98490953   33
# 0.97020745   49
# 0.94675821   68
# 0.91209573   90
# 0.86408037  115
# 0.80136335  143
# 0.72390139  174
# 0.63340271  208
# 0.53354216  245
# 0.42977926  285
# 0.32867709  328
# 0.23678841  374
# 0.15936618  423
# 0.09931426  475
# 0.05676712  530
# 0.02946457  588
# 0.01374107  649
# 0.00569389  713
# 0.00207191  780
# 0.00065396  850
# 0.00017675  923
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:929.289255 = 929.272792 + 0.016463
# loss_var     : 1045.009543 => 929.272792
# model.learning_portion: 0.01
# model.out_channels    : 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999883:   0: 0.999883; 0.000117*1674.861796= 0.196014;  0.196014/0.999883=  0.196037
0.999441:   3: 0.999558; 0.000165*1241.681815= 0.204459;  0.204459/0.999441=  0.204573
0.998555:   7: 0.999114; 0.000206* 990.716849= 0.204570;  0.204570/0.998555=  0.204866
0.997115:  12: 0.998557; 0.000248* 820.926169= 0.203297;  0.203297/0.997115=  0.203885
0.992181:  22: 0.995053; 0.001214* 608.990308= 0.739170;  0.739170/0.992181=  0.744994
0.978110:  41: 0.985817; 0.003619* 431.623901= 1.562163;  1.562163/0.978110=  1.597124
0.952069:  64: 0.973376; 0.005323* 322.659400= 1.717663;  1.717663/0.952069=  1.804138
0.911458:  91: 0.957345; 0.006947* 251.082492= 1.744213;  1.744213/0.911458=  1.913651
0.854406: 120: 0.937405; 0.008737* 200.309158= 1.750098;  1.750098/0.854406=  2.048322
0.780352: 152: 0.913327; 0.010818* 162.307348= 1.755800;  1.755800/0.780352=  2.250010
0.690592: 187: 0.884975; 0.013307* 132.164634= 1.758720;  1.758720/0.690592=  2.546686
0.588614: 225: 0.852333; 0.016348* 107.980641= 1.765226;  1.765226/0.588614=  2.998952
0.480022: 265: 0.815512; 0.020130*  87.363932= 1.758639;  1.758639/0.480022=  3.663666
0.371897: 309: 0.774749; 0.024908*  69.986408= 1.743225;  1.743225/0.371897=  4.687391
0.271641: 355: 0.730420; 0.031014*  54.953265= 1.704304;  1.704304/0.271641=  6.274110
0.185537: 405: 0.683025; 0.038867*  41.943770= 1.630239;  1.630239/0.185537=  8.786577
0.117478: 457: 0.633176; 0.048976*  30.578292= 1.497600;  1.497600/0.117478= 12.747930
0.068323: 512: 0.581585; 0.061907*  21.144744= 1.309007;  1.309007/0.068323= 19.158995
0.036146: 570: 0.529038; 0.078231*  13.539055= 1.059172;  1.059172/0.036146= 29.302881
0.017218: 631: 0.476355; 0.098444*   7.851727= 0.772952;  0.772952/0.017218= 44.891627
0.007307: 695: 0.424367; 0.122877*   4.029114= 0.495085;  0.495085/0.007307= 67.756437
0.002732: 762: 0.373880; 0.151643*   1.779020= 0.269776;  0.269776/0.002732= 98.751030
0.000890: 831: 0.325629; 0.184639*   0.652681= 0.120510;  0.120510/0.000890=135.468586
0.000249: 904: 0.280275; 0.221560*   0.206839= 0.045827;  0.045827/0.000249=183.804071
0.000059: 979: 0.238344; 0.261966*   0.067433= 0.017665;  0.017665/0.000059=297.266252
