# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-050-logSNR.txt
# order     : 1
# steps     : 50
# skip_type : logSNR
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99985278    0
# 0.99978334    0
# 0.99968112    1
# 0.99953079    2
# 0.99930942    3
# 0.99898380    5
# 0.99850506    7
# 0.99780113   10
# 0.99676692   13
# 0.99524850   16
# 0.99302185   21
# 0.98976260   26
# 0.98500395   33
# 0.97808242   41
# 0.96806973   51
# 0.95369953   63
# 0.93330741   77
# 0.90483028   94
# 0.86594027  114
# 0.81441766  138
# 0.74883711  164
# 0.66948676  195
# 0.57915610  228
# 0.48319510  264
# 0.38845766  302
# 0.30145997  341
# 0.22672275  380
# 0.16610815  418
# 0.11920083  455
# 0.08420218  492
# 0.05879358  526
# 0.04071138  560
# 0.02802482  592
# 0.01921250  623
# 0.01313375  652
# 0.00896071  680
# 0.00610539  708
# 0.00415609  734
# 0.00282739  760
# 0.00192265  784
# 0.00130704  808
# 0.00088837  832
# 0.00060372  854
# 0.00041025  876
# 0.00027875  898
# 0.00018940  919
# 0.00012869  939
# 0.00008743  959
# 0.00005940  979
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:421.228429 = 421.215716 + 0.012713
# loss_var     : 480.277573 => 421.215716
# model.learning_portion: 0.01
# model.out_channels    : 50
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999973:   0: 0.999973; 0.000027*1836.477060= 0.049031;  0.049031/0.999973=  0.049032
0.999885:   0: 0.999912; 0.000031*1678.838652= 0.051678;  0.051678/0.999885=  0.051684
0.999726:   1: 0.999841; 0.000034*1439.312609= 0.049073;  0.049073/0.999726=  0.049086
0.999477:   3: 0.999751; 0.000040*1257.747979= 0.050109;  0.050109/0.999477=  0.050135
0.999095:   5: 0.999618; 0.000052*1111.445408= 0.057916;  0.057916/0.999095=  0.057968
0.998532:   7: 0.999436; 0.000068* 986.702879= 0.067142;  0.067142/0.998532=  0.067241
0.997693:  10: 0.999160; 0.000095* 874.469902= 0.082857;  0.082857/0.997693=  0.083049
0.996485:  13: 0.998789; 0.000127* 776.108878= 0.098822;  0.098822/0.996485=  0.099170
0.994771:  17: 0.998280; 0.000171* 689.838551= 0.117910;  0.117910/0.994771=  0.118530
0.992446:  22: 0.997663; 0.000216* 615.654732= 0.132802;  0.132802/0.992446=  0.133813
0.989450:  27: 0.996982; 0.000254* 552.981509= 0.140295;  0.140295/0.989450=  0.141791
0.985778:  32: 0.996289; 0.000280* 500.200684= 0.140081;  0.140081/0.985778=  0.142102
0.981386:  38: 0.995545; 0.000304* 456.593800= 0.138900;  0.138900/0.981386=  0.141535
0.976216:  43: 0.994732; 0.000329* 419.051154= 0.138019;  0.138019/0.976216=  0.141381
0.970230:  49: 0.993868; 0.000353* 386.380773= 0.136473;  0.136473/0.970230=  0.140661
0.963490:  55: 0.993053; 0.000366* 358.540114= 0.131295;  0.131295/0.963490=  0.136271
0.952334:  64: 0.988422; 0.000804* 323.353554= 0.260038;  0.260038/0.952334=  0.273053
0.932715:  78: 0.979399; 0.001877* 281.912703= 0.529263;  0.529263/0.932715=  0.567443
0.901889:  96: 0.966950; 0.003382* 240.079100= 0.811962;  0.811962/0.901889=  0.900291
0.857188: 118: 0.950436; 0.005262* 202.140262= 1.063631;  1.063631/0.857188=  1.240837
0.796690: 145: 0.929423; 0.007495* 169.156520= 1.267822;  1.267822/0.796690=  1.591361
0.720199: 175: 0.903988; 0.010051* 141.145953= 1.418682;  1.418682/0.720199=  1.969848
0.630197: 209: 0.875033; 0.012838* 116.889680= 1.500653;  1.500653/0.630197=  2.381242
0.532056: 246: 0.844269; 0.015701*  96.772363= 1.519439;  1.519439/0.532056=  2.855787
0.433040: 284: 0.813899; 0.018450*  79.701360= 1.470461;  1.470461/0.433040=  3.395670
0.340374: 323: 0.786011; 0.020913*  65.145721= 1.362389;  1.362389/0.340374=  4.002621
0.259382: 362: 0.762048; 0.022983*  53.068400= 1.219681;  1.219681/0.259382=  4.702264
0.192622: 400: 0.742619; 0.024625*  42.883695= 1.056017;  1.056017/0.192622=  5.482331
0.140148: 438: 0.727583; 0.025869*  34.428009= 0.890623;  0.890623/0.140148=  6.354860
0.100397: 474: 0.716365; 0.026777*  27.428784= 0.734468;  0.734468/0.100397=  7.315604
0.071103: 508: 0.708217; 0.027423*  21.683116= 0.594613;  0.594613/0.071103=  8.362677
0.049945: 542: 0.702423; 0.027871*  16.982959= 0.473336;  0.473336/0.049945=  9.477234
0.034879: 573: 0.698355; 0.028179*  13.189525= 0.371665;  0.371665/0.034879= 10.655849
0.024259: 604: 0.695531; 0.028387*  10.138811= 0.287809;  0.287809/0.024259= 11.863784
0.016826: 633: 0.693583; 0.028527*   7.712321= 0.220008;  0.220008/0.016826= 13.075534
0.011648: 661: 0.692245; 0.028620*   5.835442= 0.167013;  0.167013/0.011648= 14.338722
0.008052: 688: 0.691332; 0.028682*   4.344018= 0.124597;  0.124597/0.008052= 15.473229
0.005562: 714: 0.690709; 0.028723*   3.234363= 0.092901;  0.092901/0.005562= 16.703256
0.003839: 739: 0.690284; 0.028750*   2.359588= 0.067839;  0.067839/0.003839= 17.669719
0.002649: 764: 0.689995; 0.028768*   1.733034= 0.049856;  0.049856/0.002649= 18.820110
0.001827: 788: 0.689797; 0.028780*   1.244895= 0.035828;  0.035828/0.001827= 19.606766
0.001260: 810: 0.689666; 0.028787*   0.893381= 0.025718;  0.025718/0.001260= 20.406920
0.000869: 833: 0.689568; 0.028793*   0.640254= 0.018435;  0.018435/0.000869= 21.213631
0.000599: 855: 0.689524; 0.028793*   0.453917= 0.013070;  0.013070/0.000599= 21.811508
0.000413: 876: 0.689450; 0.028802*   0.324295= 0.009340;  0.009340/0.000413= 22.608682
0.000285: 897: 0.689449; 0.028798*   0.232462= 0.006694;  0.006694/0.000285= 23.502868
0.000196: 917: 0.689450; 0.028794*   0.168454= 0.004851;  0.004851/0.000196= 24.700049
0.000135: 937: 0.689372; 0.028808*   0.123638= 0.003562;  0.003562/0.000135= 26.310134
0.000093: 956: 0.689389; 0.028803*   0.091870= 0.002646;  0.002646/0.000093= 28.353472
0.000064: 975: 0.689449; 0.028790*   0.070680= 0.002035;  0.002035/0.000064= 31.624912
