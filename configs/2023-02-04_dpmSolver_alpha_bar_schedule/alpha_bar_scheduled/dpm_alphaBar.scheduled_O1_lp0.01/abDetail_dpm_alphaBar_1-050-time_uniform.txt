# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-050-time_uniform.txt
# order     : 1
# steps     : 50
# skip_type : time_uniform
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99374515   19
# 0.97980201   39
# 0.95839578   59
# 0.93002242   79
# 0.89532876   99
# 0.85508835  119
# 0.81017196  139
# 0.76151741  159
# 0.71009684  179
# 0.65688461  199
# 0.60282701  219
# 0.54881662  239
# 0.49566856  259
# 0.44410270  279
# 0.39473221  299
# 0.34805411  319
# 0.30444932  339
# 0.26418364  359
# 0.22741456  379
# 0.19420029  399
# 0.16451307  419
# 0.13825080  439
# 0.11525249  459
# 0.09531191  479
# 0.07819104  499
# 0.06363231  519
# 0.05136989  539
# 0.04113851  559
# 0.03268097  579
# 0.02575416  599
# 0.02013281  619
# 0.01561221  639
# 0.01200952  659
# 0.00916405  679
# 0.00693662  699
# 0.00520841  719
# 0.00387935  739
# 0.00286620  759
# 0.00210062  779
# 0.00152714  799
# 0.00110129  819
# 0.00078780  839
# 0.00055900  859
# 0.00039346  879
# 0.00027471  899
# 0.00019025  919
# 0.00013069  939
# 0.00008906  959
# 0.00006020  979
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:350.927955 = 350.921176 + 0.006779
# loss_var     : 416.535332 => 350.921176
# model.learning_portion: 0.01
# model.out_channels    : 50
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.998882:   5: 0.998882; 0.001118*1055.848856= 1.180117;  1.180117/0.998882=  1.181437
0.994710:  17: 0.995823; 0.001550* 687.382474= 1.065525;  1.065525/0.994710=  1.071191
0.982890:  36: 0.988117; 0.003423* 469.927566= 1.608505;  1.608505/0.982890=  1.636505
0.963581:  55: 0.980355; 0.003761* 358.835971= 1.349472;  1.349472/0.963581=  1.400477
0.937230:  75: 0.972654; 0.003885* 289.865034= 1.126036;  1.126036/0.937230=  1.201451
0.904438:  94: 0.965011; 0.003971* 242.974150= 0.964827;  0.964827/0.904438=  1.066771
0.865932: 114: 0.957426; 0.004054* 208.380912= 0.844838;  0.844838/0.865932=  0.975640
0.822546: 134: 0.949896; 0.004146* 181.550135= 0.752741;  0.752741/0.822546=  0.915136
0.775192: 154: 0.942430; 0.004250* 160.137799= 0.680573;  0.680573/0.775192=  0.877942
0.724819: 174: 0.935018; 0.004369* 142.287132= 0.621721;  0.621721/0.724819=  0.857760
0.672383: 194: 0.927657; 0.004507* 127.356391= 0.573957;  0.573957/0.672383=  0.853616
0.618834: 213: 0.920359; 0.004661* 114.398861= 0.533240;  0.533240/0.618834=  0.861685
0.565068: 233: 0.913118; 0.004835* 102.976836= 0.497923;  0.497923/0.565068=  0.881173
0.511907: 253: 0.905920; 0.005031*  93.021404= 0.468026;  0.468026/0.511907=  0.914280
0.460094: 273: 0.898785; 0.005248*  84.016241= 0.440949;  0.440949/0.460094=  0.958389
0.410268: 293: 0.891705; 0.005488*  75.978703= 0.416999;  0.416999/0.410268=  1.016406
0.362955: 313: 0.884678; 0.005753*  68.777036= 0.395657;  0.395657/0.362955=  1.090100
0.318564: 333: 0.877696; 0.006044*  61.889790= 0.374034;  0.374034/0.318564=  1.174124
0.277400: 352: 0.870782; 0.006359*  55.773746= 0.354686;  0.354686/0.277400=  1.278607
0.239649: 372: 0.863911; 0.006704*  50.158994= 0.336269;  0.336269/0.239649=  1.403173
0.205401: 392: 0.857092; 0.007078*  44.866767= 0.317552;  0.317552/0.205401=  1.546006
0.174658: 412: 0.850326; 0.007481*  40.106646= 0.300041;  0.300041/0.174658=  1.717874
0.147343: 432: 0.843611; 0.007915*  35.622398= 0.281959;  0.281959/0.147343=  1.913617
0.123319: 452: 0.836949; 0.008381*  31.545903= 0.264378;  0.264378/0.123319=  2.143859
0.102396: 472: 0.830334; 0.008879*  27.838971= 0.247172;  0.247172/0.102396=  2.413887
0.084351: 491: 0.823770; 0.009409*  24.412262= 0.229697;  0.229697/0.084351=  2.723122
0.068937: 511: 0.817264; 0.009971*  21.238455= 0.211779;  0.211779/0.068937=  3.072068
0.055894: 531: 0.810798; 0.010568*  18.412388= 0.194575;  0.194575/0.055894=  3.481154
0.044960: 551: 0.804385; 0.011196*  15.752192= 0.176363;  0.176363/0.044960=  3.922657
0.035879: 571: 0.798019; 0.011857*  13.448920= 0.159465;  0.159465/0.035879=  4.444516
0.028406: 591: 0.791704; 0.012550*  11.345359= 0.142381;  0.142381/0.028406=  5.012416
0.022311: 611: 0.785435; 0.013274*   9.520411= 0.126372;  0.126372/0.022311=  5.664171
0.017385: 630: 0.779214; 0.014028*   7.927087= 0.111204;  0.111204/0.017385=  6.396600
0.013439: 650: 0.773044; 0.014812*   6.513436= 0.096477;  0.096477/0.013439=  7.178773
0.010307: 670: 0.766916; 0.015625*   5.288162= 0.082628;  0.082628/0.010307=  8.016900
0.007842: 690: 0.760839; 0.016465*   4.268175= 0.070277;  0.070277/0.007842=  8.961851
0.005919: 710: 0.754803; 0.017333*   3.404729= 0.059014;  0.059014/0.005919=  9.970318
0.004432: 730: 0.748816; 0.018226*   2.672387= 0.048708;  0.048708/0.004432= 10.989366
0.003293: 750: 0.742876; 0.019144*   2.072421= 0.039675;  0.039675/0.003293= 12.049582
0.002427: 770: 0.736977; 0.020087*   1.609489= 0.032329;  0.032329/0.002427= 13.322913
0.001774: 789: 0.731129; 0.021051*   1.222183= 0.025728;  0.025728/0.001774= 14.501741
0.001287: 809: 0.725327; 0.022038*   0.914372= 0.020151;  0.020151/0.001287= 15.659018
0.000926: 829: 0.719556; 0.023048*   0.676427= 0.015590;  0.015590/0.000926= 16.837264
0.000661: 849: 0.713851; 0.024076*   0.497943= 0.011988;  0.011988/0.000661= 18.137045
0.000468: 869: 0.708176; 0.025126*   0.363021= 0.009121;  0.009121/0.000468= 19.485950
0.000329: 889: 0.702536; 0.026198*   0.265203= 0.006948;  0.006948/0.000329= 21.127136
0.000229: 909: 0.696927; 0.027292*   0.191836= 0.005235;  0.005235/0.000229= 22.843654
0.000158: 928: 0.691449; 0.028386*   0.140405= 0.003986;  0.003986/0.000158= 25.150067
0.000109: 948: 0.685939; 0.029514*   0.103946= 0.003068;  0.003068/0.000109= 28.223141
0.000074: 968: 0.680423; 0.030671*   0.078130= 0.002396;  0.002396/0.000074= 32.398647
