# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-025-time_uniform.txt
# order     : 1
# steps     : 25
# skip_type : time_uniform
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.97980201   39
# 0.93002242   79
# 0.85508835  119
# 0.76151741  159
# 0.65688461  199
# 0.54881668  239
# 0.44410276  279
# 0.34805417  319
# 0.26418364  359
# 0.19420020  399
# 0.13825075  439
# 0.09531189  479
# 0.06363226  519
# 0.04113851  559
# 0.02575416  599
# 0.01561221  639
# 0.00916405  679
# 0.00520841  719
# 0.00286620  759
# 0.00152714  799
# 0.00078780  839
# 0.00039346  879
# 0.00019025  919
# 0.00008906  959
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:657.557903 = 657.541175 + 0.016728
# loss_var     : 738.407760 => 657.541175
# model.learning_portion: 0.01
# model.out_channels    : 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.989783:  26: 0.989783; 0.010217* 558.849470= 5.709731;  5.709731/0.989783=  5.768669
0.949378:  66: 0.959178; 0.015875* 315.989575= 5.016492;  5.016492/0.949378=  5.283975
0.882363: 106: 0.929411; 0.015895* 221.306393= 3.517742;  3.517742/0.882363=  3.986730
0.794615: 146: 0.900554; 0.016310* 168.275299= 2.744612;  2.744612/0.794615=  3.454013
0.693368: 186: 0.872583; 0.017006* 132.854827= 2.259268;  2.259268/0.693368=  3.258395
0.586220: 225: 0.845468; 0.017981* 107.333530= 1.929978;  1.929978/0.586220=  3.292240
0.480223: 265: 0.819186; 0.019252*  87.397858= 1.682559;  1.682559/0.480223=  3.503700
0.381158: 305: 0.793710; 0.020841*  71.365155= 1.487296;  1.487296/0.381158=  3.902044
0.293117: 345: 0.769017; 0.022773*  58.200058= 1.325410;  1.325410/0.293117=  4.521775
0.218396: 384: 0.745082; 0.025075*  46.959530= 1.177528;  1.177528/0.218396=  5.391699
0.157657: 424: 0.721886; 0.027769*  37.466513= 1.040413;  1.040413/0.157657=  6.599203
0.110266: 464: 0.699401; 0.030873*  29.289725= 0.904248;  0.904248/0.110266=  8.200628
0.074717: 503: 0.677610; 0.034393*  22.484018= 0.773299;  0.773299/0.074717= 10.349687
0.049051: 543: 0.656494; 0.038330*  16.749587= 0.642007;  0.642007/0.049051= 13.088458
0.031198: 583: 0.636026; 0.042671*  12.149274= 0.518428;  0.518428/0.031198= 16.617373
0.019224: 623: 0.616193; 0.047395*   8.556803= 0.405548;  0.405548/0.019224= 21.095971
0.011476: 662: 0.596972; 0.052472*   5.758182= 0.302146;  0.302146/0.011476= 26.328150
0.006637: 702: 0.578346; 0.057870*   3.734324= 0.216105;  0.216105/0.006637= 32.559815
0.003719: 742: 0.560296; 0.063553*   2.307291= 0.146636;  0.146636/0.003719= 39.431225
0.002019: 781: 0.542804; 0.069490*   1.364097= 0.094791;  0.094791/0.002019= 46.959434
0.001061: 821: 0.525860; 0.075646*   0.765277= 0.057891;  0.057891/0.001061= 54.537249
0.000541: 861: 0.509436; 0.082002*   0.413613= 0.033917;  0.033917/0.000541= 62.721399
0.000267: 900: 0.493525; 0.088532*   0.219099= 0.019397;  0.019397/0.000267= 72.681841
0.000128: 940: 0.478116; 0.095215*   0.117832= 0.011219;  0.011219/0.000128= 87.926655
0.000059: 979: 0.463174; 0.102045*   0.067229= 0.006860;  0.006860/0.000059=116.080848
