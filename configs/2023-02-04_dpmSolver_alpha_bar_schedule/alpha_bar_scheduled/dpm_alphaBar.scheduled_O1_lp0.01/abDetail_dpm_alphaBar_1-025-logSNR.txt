# Old comments in file ../exp/dpm_alphaBar/dpm_alphaBar_1-025-logSNR.txt
# order     : 1
# steps     : 25
# skip_type : logSNR
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99978334    0
# 0.99953079    2
# 0.99898380    5
# 0.99780113   10
# 0.99524850   16
# 0.98976260   26
# 0.97808242   41
# 0.95369953   63
# 0.90483028   94
# 0.81441766  138
# 0.66948676  195
# 0.48319510  264
# 0.30145997  341
# 0.16610815  418
# 0.08420218  492
# 0.04071138  560
# 0.01921250  623
# 0.00896071  680
# 0.00415609  734
# 0.00192265  784
# 0.00088837  832
# 0.00041025  876
# 0.00018940  919
# 0.00008743  959
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000.0
# beta_schedule: linear
# torch.seed() : 8079376260256798063
# alpha_bar_dir: ../exp/dpm_alphaBar
# Epoch        : 009999; loss:763.628084 = 763.608619 + 0.019464
# loss_var     : 838.206668 => 763.608619
# model.learning_portion: 0.01
# model.out_channels    : 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999842:   0: 0.999842; 0.000158*1601.956055= 0.252762;  0.252762/0.999842=  0.252802
0.999379:   3: 0.999537; 0.000153*1214.695837= 0.185340;  0.185340/0.999379=  0.185455
0.998355:   8: 0.998975; 0.000245* 957.649393= 0.234727;  0.234727/0.998355=  0.235113
0.996200:  14: 0.997841; 0.000447* 758.841398= 0.338975;  0.338975/0.996200=  0.340268
0.992608:  22: 0.996395; 0.000597* 619.757497= 0.370231;  0.370231/0.992608=  0.372989
0.987405:  30: 0.994758; 0.000701* 521.936361= 0.365870;  0.365870/0.987405=  0.370537
0.980397:  39: 0.992903; 0.000794* 448.464128= 0.356186;  0.356186/0.980397=  0.363308
0.965656:  53: 0.984964; 0.002150* 366.312981= 0.787611;  0.787611/0.965656=  0.815623
0.925788:  82: 0.958714; 0.008274* 270.380663= 2.237190;  2.237190/0.925788=  2.416525
0.842511: 125: 0.910047; 0.018761* 192.795695= 3.617071;  3.617071/0.842511=  4.293203
0.700986: 183: 0.832020; 0.034164* 135.072787= 4.614660;  4.614660/0.700986=  6.583101
0.512925: 253: 0.731719; 0.052971*  93.279955= 4.941089;  4.941089/0.512925=  9.633170
0.325129: 329: 0.633873; 0.070680*  62.845286= 4.441923;  4.441923/0.325129= 13.662023
0.182397: 407: 0.561000; 0.083467*  41.354269= 3.451732;  3.451732/0.182397= 18.924241
0.094281: 480: 0.516900; 0.090963*  26.323849= 2.394505;  2.394505/0.094281= 25.397489
0.046526: 548: 0.493484; 0.094809*  16.171033= 1.533158;  1.533158/0.046526= 32.952516
0.022421: 610: 0.481910; 0.096640*   9.567225= 0.924576;  0.924576/0.022421= 41.236136
0.010681: 668: 0.476392; 0.097478*   5.461308= 0.532359;  0.532359/0.010681= 49.839672
0.005061: 721: 0.473805; 0.097855*   2.992506= 0.292831;  0.292831/0.005061= 57.861301
0.002392: 770: 0.472603; 0.098022*   1.580310= 0.154904;  0.154904/0.002392= 64.764710
0.001129: 817: 0.472048; 0.098094*   0.812505= 0.079702;  0.079702/0.001129= 70.592284
0.000533: 861: 0.471794; 0.098125*   0.409151= 0.040148;  0.040148/0.000533= 75.370032
0.000251: 903: 0.471663; 0.098144*   0.208181= 0.020432;  0.020432/0.000251= 81.322783
0.000118: 944: 0.471609; 0.098150*   0.111416= 0.010936;  0.010936/0.000118= 92.291390
0.000056: 982: 0.471621; 0.098136*   0.064649= 0.006344;  0.006344/0.000056=113.531951
