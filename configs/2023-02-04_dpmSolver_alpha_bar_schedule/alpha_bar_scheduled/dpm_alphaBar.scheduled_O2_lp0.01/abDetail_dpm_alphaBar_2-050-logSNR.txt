# Old comments in file ./exp/dpm_alphaBar/dpm_alphaBar_2-050-logSNR.txt
# order     : 2
# steps     : 50
# skip_type : logSNR
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99985278    0
# 0.99978334    0
# 0.99968112    1
# 0.99953079    2
# 0.99930942    3
# 0.99898380    5
# 0.99850500    7
# 0.99780113   10
# 0.99676692   13
# 0.99524850   16
# 0.99302185   21
# 0.98976260   26
# 0.98500395   33
# 0.97808242   41
# 0.96806973   51
# 0.95369953   63
# 0.93330741   77
# 0.90483028   94
# 0.86594027  114
# 0.81441766  138
# 0.74883711  164
# 0.66948676  195
# 0.57915604  228
# 0.48319510  264
# 0.38845766  302
# 0.30145997  341
# 0.22672275  380
# 0.16610815  418
# 0.11920086  455
# 0.08420218  492
# 0.05879358  526
# 0.04071138  560
# 0.02802482  592
# 0.01921250  623
# 0.01313374  652
# 0.00896071  680
# 0.00610539  708
# 0.00415609  734
# 0.00282739  760
# 0.00192265  784
# 0.00130704  808
# 0.00088837  832
# 0.00060372  854
# 0.00041025  876
# 0.00027876  898
# 0.00018940  919
# 0.00012869  939
# 0.00008743  959
# 0.00005940  979
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000
# beta_schedule: linear
# torch.seed() : 11898236869720319207
# alpha_bar_dir: ./exp/dpm_alphaBar
# Epoch        : 009999; loss:981.750998 = 981.738260 + 0.012738
# loss_var     : 1123.885328 => 981.738260
# model.learning_portion: 0.01
# model.out_channels    : 50
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999780:   1: 0.999780; 0.0     *   0.0     = 0.0     ;  0.0     /0.999780=  0.0     
0.999760:   1: 0.999980; 0.000240*1490.746944= 0.357099;  0.357099/0.999760=  0.357184
0.999576:   2: 0.999815; 0.0     *   0.0     = 0.0     ;  0.0     /0.999576=  0.0     
0.999292:   4: 0.999715; 0.000124*1318.022819= 0.163685;  0.163685/0.999292=  0.163802
0.998871:   6: 0.999579; 0.0     *   0.0     = 0.0     ;  0.0     /0.998871=  0.0     
0.998251:   8: 0.999379; 0.000232*1053.528180= 0.244136;  0.244136/0.998251=  0.244564
0.997346:  11: 0.999094; 0.0     *   0.0     = 0.0     ;  0.0     /0.997346=  0.0     
0.996041:  14: 0.998692; 0.000447* 840.715298= 0.375746;  0.375746/0.996041=  0.377239
0.994116:  19: 0.998067; 0.0     *   0.0     = 0.0     ;  0.0     /0.994116=  0.0     
0.991991:  23: 0.997863; 0.000713* 665.028409= 0.474120;  0.474120/0.991991=  0.477948
0.987926:  29: 0.995902; 0.0     *   0.0     = 0.0     ;  0.0     /0.987926=  0.0     
0.985533:  32: 0.997578; 0.000966* 529.333895= 0.511173;  0.511173/0.985533=  0.518677
0.976882:  43: 0.991222; 0.0     *   0.0     = 0.0     ;  0.0     /0.976882=  0.0     
0.975549:  44: 0.998636; 0.001347* 423.394563= 0.570287;  0.570287/0.975549=  0.584580
0.967646:  52: 0.991898; 0.0     *   0.0     = 0.0     ;  0.0     /0.967646=  0.0     
0.962107:  56: 0.994277; 0.001550* 374.516730= 0.580623;  0.580623/0.962107=  0.603491
0.951105:  65: 0.988564; 0.0     *   0.0     = 0.0     ;  0.0     /0.951105=  0.0     
0.931559:  78: 0.979449; 0.004909* 320.058758= 1.571308;  1.571308/0.931559=  1.686752
0.900811:  96: 0.966993; 0.0     *   0.0     = 0.0     ;  0.0     /0.900811=  0.0     
0.856205: 119: 0.950483; 0.016485* 238.764055= 3.936033;  3.936033/0.856205=  4.597067
0.795807: 145: 0.929458; 0.0     *   0.0     = 0.0     ;  0.0     /0.795807=  0.0     
0.719425: 176: 0.904020; 0.033159* 168.851871= 5.599000;  5.599000/0.719425=  7.782604
0.629539: 209: 0.875059; 0.0     *   0.0     = 0.0     ;  0.0     /0.629539=  0.0     
0.531517: 246: 0.844295; 0.052517* 116.795261= 6.133721;  6.133721/0.531517= 11.540021
0.432614: 284: 0.813922; 0.0     *   0.0     = 0.0     ;  0.0     /0.432614=  0.0     
0.340048: 323: 0.786032; 0.070176*  79.656614= 5.589955;  5.589955/0.340048= 16.438719
0.259141: 362: 0.762070; 0.0     *   0.0     = 0.0     ;  0.0     /0.259141=  0.0     
0.192448: 400: 0.742640; 0.082654*  53.025702= 4.382789;  4.382789/0.192448= 22.773884
0.140025: 438: 0.727601; 0.0     *   0.0     = 0.0     ;  0.0     /0.140025=  0.0     
0.100311: 474: 0.716380; 0.089838*  34.409852= 3.091312;  3.091312/0.100311= 30.817151
0.071044: 508: 0.708235; 0.0     *   0.0     = 0.0     ;  0.0     /0.071044=  0.0     
0.049904: 542: 0.702438; 0.093458*  21.673262= 2.025542;  2.025542/0.049904= 40.588745
0.034851: 573: 0.698370; 0.0     *   0.0     = 0.0     ;  0.0     /0.034851=  0.0     
0.024241: 604: 0.695546; 0.095149*  13.183315= 1.254374;  1.254374/0.024241= 51.746405
0.016813: 633: 0.693596; 0.0     *   0.0     = 0.0     ;  0.0     /0.016813=  0.0     
0.011639: 661: 0.692260; 0.095905*   7.706867= 0.739128;  0.739128/0.011639= 63.503268
0.008047: 688: 0.691344; 0.0     *   0.0     = 0.0     ;  0.0     /0.008047=  0.0     
0.005558: 714: 0.690720; 0.096235*   4.340008= 0.417662;  0.417662/0.005558= 75.145963
0.003837: 739: 0.690295; 0.0     *   0.0     = 0.0     ;  0.0     /0.003837=  0.0     
0.002647: 764: 0.690004; 0.096377*   2.357089= 0.227169;  0.227169/0.002647= 85.811074
0.001826: 788: 0.689806; 0.0     *   0.0     = 0.0     ;  0.0     /0.001826=  0.0     
0.001259: 811: 0.689677; 0.096437*   1.244107= 0.119977;  0.119977/0.001259= 95.262194
0.000868: 833: 0.689577; 0.0     *   0.0     = 0.0     ;  0.0     /0.000868=  0.0     
0.000599: 855: 0.689533; 0.096460*   0.639796= 0.061715;  0.061715/0.000599=103.055775
0.000413: 876: 0.689483; 0.0     *   0.0     = 0.0     ;  0.0     /0.000413=  0.0     
0.000285: 897: 0.689434; 0.096476*   0.324129= 0.031271;  0.031271/0.000285=109.850660
0.000196: 917: 0.689457; 0.0     *   0.0     = 0.0     ;  0.0     /0.000196=  0.0     
0.000135: 937: 0.689381; 0.096480*   0.168370= 0.016244;  0.016244/0.000135=120.060371
0.000093: 956: 0.689396; 0.0     *   0.0     = 0.0     ;  0.0     /0.000093=  0.0     
0.000064: 975: 0.689458; 0.096465*   0.091833= 0.008859;  0.008859/0.000064=137.750123
