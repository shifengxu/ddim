# Old comments in file ./exp/dpm_alphaBar/dpm_alphaBar_2-025-logSNR.txt
# order     : 2
# steps     : 25
# skip_type : logSNR
# data_type : alpha_bar
# 
# Old alpha_bar and its timestep
# 0.99955779    2
# 0.99907047    5
# 0.99804723    9
# 0.99590206   15
# 0.99142104   24
# 0.98212731   37
# 0.96314025   56
# 0.92551154   82
# 0.85524255  119
# 0.73748755  169
# 0.57189232  231
# 0.38845772  302
# 0.23197812  377
# 0.12558745  450
# 0.06392859  519
# 0.03145308  582
# 0.01520700  641
# 0.00728917  695
# 0.00347935  746
# 0.00165748  794
# 0.00078883  839
# 0.00037525  881
# 0.00017847  922
# 0.00008487  961
# 0.00004036  998
# 
# lr           : 1e-06
# n_epochs     : 10000
# aa_low       : 0.0001
# aa_low_lambda: 10000000
# beta_schedule: linear
# torch.seed() : 11898236869720319207
# alpha_bar_dir: ./exp/dpm_alphaBar
# Epoch        : 009999; loss:2019.627293 = 2019.607989 + 0.019304
# loss_var     : 2223.131399 => 2019.607989
# model.learning_portion: 0.01
# model.out_channels    : 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999128:   4: 0.999128; 0.000872*1121.876768= 0.978558;  0.978558/0.999128=  0.979412
0.998174:   8: 0.999046; 0.0     *   0.0     = 0.0     ;  0.0     /0.998174=  0.0     
0.996211:  14: 0.998033; 0.001028* 932.024857= 0.958162;  0.958162/0.996211=  0.961806
0.992000:  23: 0.995773; 0.0     *   0.0     = 0.0     ;  0.0     /0.992000=  0.0     
0.988149:  29: 0.996118; 0.002262* 604.660574= 1.367663;  1.367663/0.988149=  1.384066
0.979703:  40: 0.991453; 0.0     *   0.0     = 0.0     ;  0.0     /0.979703=  0.0     
0.970442:  49: 0.990547; 0.004101* 443.055233= 1.817053;  1.817053/0.970442=  1.872397
0.942217:  71: 0.970916; 0.0     *   0.0     = 0.0     ;  0.0     /0.942217=  0.0     
0.880092: 107: 0.934065; 0.033325* 299.579995= 9.983499;  9.983499/0.880092= 11.343694
0.767709: 157: 0.872305; 0.0     *   0.0     = 0.0     ;  0.0     /0.767709=  0.0     
0.603001: 219: 0.785454; 0.117959* 157.092026=18.530372; 18.530372/0.603001= 30.730270
0.415615: 291: 0.689245; 0.0     *   0.0     = 0.0     ;  0.0     /0.415615=  0.0     
0.252350: 365: 0.607173; 0.208907*  76.822731=16.048838; 16.048838/0.252350= 63.597465
0.139139: 438: 0.551373; 0.0     *   0.0     = 0.0     ;  0.0     /0.139139=  0.0     
0.072218: 507: 0.519034; 0.250653*  34.279026= 8.592152;  8.592152/0.072218=118.975426
0.036253: 570: 0.502000; 0.0     *   0.0     = 0.0     ;  0.0     /0.036253=  0.0     
0.017890: 628: 0.493480; 0.261736*  13.567212= 3.551029;  3.551029/0.017890=198.488820
0.008754: 682: 0.489327; 0.0     *   0.0     = 0.0     ;  0.0     /0.008754=  0.0     
0.004266: 732: 0.487329; 0.264119*   4.674811= 1.234707;  1.234707/0.004266=289.416709
0.002075: 779: 0.486373; 0.0     *   0.0     = 0.0     ;  0.0     /0.002075=  0.0     
0.001008: 824: 0.485920; 0.264595*   1.396545= 0.369518;  0.369518/0.001008=366.489499
0.000490: 866: 0.485701; 0.0     *   0.0     = 0.0     ;  0.0     /0.000490=  0.0     
0.000238: 906: 0.485601; 0.264684*   0.378405= 0.100158;  0.100158/0.000238=421.173641
0.000115: 945: 0.485540; 0.0     *   0.0     = 0.0     ;  0.0     /0.000115=  0.0     
0.000056: 982: 0.485550; 0.264695*   0.108909= 0.028828;  0.028828/0.000056=514.194784
