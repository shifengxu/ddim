# Old comments in file ./phase1_ab_original/dpm_alphaBar_1-025-time_uniform.txt
# order     : 1
# steps     : 25
# skip_type : time_uniform
# data_type : alpha_bar
# alpha_bar : timestep
# 
# Old alpha_bar and its timestep, and estimated timestep in vs
# 0.97980201  :    0.04096 :   39
# 0.93002242  :    0.08092 :   79
# 0.85508835  :    0.12088 :  119
# 0.76151741  :    0.16084 :  159
# 0.65688461  :    0.20080 :  199
# 0.54881668  :    0.24076 :  239
# 0.44410276  :    0.28072 :  279
# 0.34805417  :    0.32068 :  319
# 0.26418364  :    0.36064 :  359
# 0.19420020  :    0.40060 :  399
# 0.13825075  :    0.44056 :  439
# 0.09531189  :    0.48052 :  479
# 0.06363226  :    0.52048 :  519
# 0.04113851  :    0.56044 :  559
# 0.02575416  :    0.60040 :  599
# 0.01561221  :    0.64036 :  639
# 0.00916405  :    0.68032 :  679
# 0.00520841  :    0.72028 :  719
# 0.00286620  :    0.76024 :  759
# 0.00152714  :    0.80020 :  799
# 0.00078780  :    0.84016 :  839
# 0.00039346  :    0.88012 :  879
# 0.00019025  :    0.92008 :  919
# 0.00008906  :    0.96004 :  959
# 0.00004036  :    1.00000 :  998
# 
# lr           : 4e-06
# lp           : 0.1
# n_epochs     : 1000
# aa_low       : 0.0001
# aa_low_lambda: 1.0e+07
# beta_schedule: linear
# torch.seed() : 9543064785745333727
# order from param    : None
# order from file     : 1
# order final         : 1
# skip_type from param: None
# skip_type from file : time_uniform
# skip_type final     : time_uniform
# Epoch       : 000999; loss:296.924820 = 284.861773 + 12.063047
# loss_var    : 717.527794 => 284.861773
# model.lp    : 0.1
# model.out_ch: 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.998384:   7: 0.998384; 0.001616* 961.709636= 1.554057;  1.554057/0.998384=  1.556573
0.992500:  22: 0.994106; 0.002164* 617.021628= 1.335515;  1.335515/0.992500=  1.345608
0.980648:  38: 0.988059; 0.002812* 450.425402= 1.266490;  1.266490/0.980648=  1.291482
0.961514:  57: 0.980488; 0.003414* 351.150815= 1.198952;  1.198952/0.961514=  1.246943
0.925266:  82: 0.962301; 0.006550* 269.554495= 1.765477;  1.765477/0.925266=  1.908076
0.865385: 114: 0.935283; 0.010510* 207.947799= 2.185465;  2.185465/0.865385=  2.525424
0.786633: 149: 0.908998; 0.012569* 164.731174= 2.070447;  2.070447/0.786633=  2.632036
0.695022: 185: 0.883540; 0.013939* 133.296321= 1.857952;  1.857952/0.695022=  2.673228
0.596926: 221: 0.858858; 0.015151* 109.439330= 1.658069;  1.658069/0.596926=  2.777681
0.498394: 258: 0.834936; 0.016415*  90.558250= 1.486476;  1.486476/0.498394=  2.982530
0.404569: 295: 0.811744; 0.017833*  75.004312= 1.337525;  1.337525/0.404569=  3.306050
0.319313: 332: 0.789268; 0.019462*  61.981045= 1.206251;  1.206251/0.319313=  3.777640
0.245066: 369: 0.767477; 0.021342*  50.928035= 1.086912;  1.086912/0.245066=  4.435189
0.182909: 406: 0.746366; 0.023499*  41.400647= 0.972861;  0.972861/0.182909=  5.318836
0.132777: 444: 0.725920; 0.025950*  33.305499= 0.864283;  0.864283/0.132777=  6.509285
0.093753: 481: 0.706092; 0.028713*  26.230729= 0.753165;  0.753165/0.093753=  8.033525
0.064398: 518: 0.686895; 0.031784*  20.179094= 0.641369;  0.641369/0.064398=  9.959414
0.043035: 555: 0.668269; 0.035167*  15.312266= 0.538484;  0.538484/0.043035= 12.512582
0.027983: 592: 0.650230; 0.038842*  11.245396= 0.436789;  0.436789/0.027983= 15.609110
0.017706: 629: 0.632749; 0.042791*   8.044060= 0.344216;  0.344216/0.017706= 19.440470
0.010904: 666: 0.615814; 0.046991*   5.524844= 0.259616;  0.259616/0.010904= 23.809901
0.006536: 703: 0.599398; 0.051416*   3.681740= 0.189301;  0.189301/0.006536= 28.964285
0.003813: 740: 0.583491; 0.056039*   2.351372= 0.131769;  0.131769/0.003813= 34.553307
0.002166: 777: 0.568083; 0.060832*   1.447595= 0.088060;  0.088060/0.002166= 40.648538
0.001198: 814: 0.553142; 0.065778*   0.857037= 0.056374;  0.056374/0.001198= 47.044059
