# Old comments in file ./phase1_ab_original/dpm_alphaBar_2-025-logSNR.txt
# order     : 2
# steps     : 25
# skip_type : logSNR
# data_type : alpha_bar
# alpha_bar : timestep
# 
# Old alpha_bar and its timestep, and estimated timestep in vs
# 0.99955779  :    0.00352 :    2
# 0.99907047  :    0.00614 :    5
# 0.99804723  :    0.01019 :    9
# 0.99590206  :    0.01628 :   15
# 0.99142104  :    0.02523 :   24
# 0.98212731  :    0.03826 :   37
# 0.96314025  :    0.05704 :   56
# 0.92551154  :    0.08373 :   82
# 0.85524255  :    0.12081 :  119
# 0.73748755  :    0.17029 :  169
# 0.57189232  :    0.23220 :  231
# 0.38845772  :    0.30332 :  302
# 0.23197812  :    0.37803 :  377
# 0.12558745  :    0.45122 :  450
# 0.06392859  :    0.52004 :  519
# 0.03145308  :    0.58368 :  582
# 0.01520700  :    0.64239 :  641
# 0.00728917  :    0.69678 :  695
# 0.00347935  :    0.74750 :  746
# 0.00165748  :    0.79512 :  794
# 0.00078883  :    0.84008 :  839
# 0.00037525  :    0.88278 :  881
# 0.00017847  :    0.92351 :  922
# 0.00008487  :    0.96252 :  961
# 0.00004036  :    1.00000 :  998
# 
# lr           : 4e-06
# lp           : 0.1
# n_epochs     : 1000
# aa_low       : 0.0001
# aa_low_lambda: 1.0e+07
# beta_schedule: linear
# torch.seed() : 16605263139796927651
# order from param    : None
# order from file     : 2
# order final         : 2
# skip_type from param: None
# skip_type from file : logSNR
# skip_type final     : logSNR
# Epoch       : 000999; loss:1521.337953 = 1517.655326 + 3.682627
# loss_var    : 3591.175681 => 1517.655326
# model.lp    : 0.1
# model.out_ch: 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999132:   4: 0.999132; 0.000868*2246.803831= 1.949590;  1.949590/0.999132=  1.951284
0.998183:   8: 0.999050; 0.0     *   0.0     = 0.0     ;  0.0     /0.998183=  0.0     
0.996188:  14: 0.998001; 0.001045*1691.487853= 1.767360;  1.767360/0.996188=  1.774122
0.992013:  23: 0.995809; 0.0     *   0.0     = 0.0     ;  0.0     /0.992013=  0.0     
0.987819:  29: 0.995772; 0.002390*1132.864109= 2.707571;  2.707571/0.987819=  2.740958
0.970209:  49: 0.982173; 0.0     *   0.0     = 0.0     ;  0.0     /0.970209=  0.0     
0.969486:  50: 0.999255; 0.004270* 768.999041= 3.283709;  3.283709/0.969486=  3.387062
0.940479:  73: 0.970081; 0.0     *   0.0     = 0.0     ;  0.0     /0.940479=  0.0     
0.939332:  73: 0.998780; 0.005530* 590.536751= 3.265725;  3.265725/0.939332=  3.476647
0.903913:  95: 0.962294; 0.0     *   0.0     = 0.0     ;  0.0     /0.903913=  0.0     
0.791328: 147: 0.875447; 0.053238* 409.122829=21.780767; 21.780767/0.791328= 27.524332
0.616634: 214: 0.779239; 0.0     *   0.0     = 0.0     ;  0.0     /0.616634=  0.0     
0.429898: 285: 0.697169; 0.175021* 193.063552=33.790271; 33.790271/0.429898= 78.600647
0.275723: 353: 0.641369; 0.0     *   0.0     = 0.0     ;  0.0     /0.275723=  0.0     
0.167924: 417: 0.609030; 0.193849*  94.551729=18.328739; 18.328739/0.167924=109.149175
0.099410: 475: 0.591997; 0.0     *   0.0     = 0.0     ;  0.0     /0.099410=  0.0     
0.058004: 528: 0.583477; 0.188752*  46.103206= 8.702073;  8.702073/0.058004=150.026371
0.033603: 577: 0.579324; 0.0     *   0.0     = 0.0     ;  0.0     /0.033603=  0.0     
0.019400: 622: 0.577326; 0.183999*  21.395057= 3.936673;  3.936673/0.019400=202.923031
0.011181: 664: 0.576371; 0.0     *   0.0     = 0.0     ;  0.0     /0.011181=  0.0     
0.006440: 704: 0.575915; 0.181687*   9.295662= 1.688904;  1.688904/0.006440=262.268683
0.003707: 742: 0.575699; 0.0     *   0.0     = 0.0     ;  0.0     /0.003707=  0.0     
0.002134: 778: 0.575598; 0.180745*   3.732837= 0.674690;  0.674690/0.002134=316.177812
0.001228: 812: 0.575535; 0.0     *   0.0     = 0.0     ;  0.0     /0.001228=  0.0     
0.000707: 845: 0.575547; 0.180387*   1.401470= 0.252807;  0.252807/0.000707=357.655202
