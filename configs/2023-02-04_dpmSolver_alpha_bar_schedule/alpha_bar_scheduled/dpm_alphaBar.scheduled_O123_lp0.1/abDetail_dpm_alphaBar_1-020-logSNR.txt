# Old comments in file ./phase1_ab_original/dpm_alphaBar_1-020-logSNR.txt
# order     : 1
# steps     : 20
# skip_type : logSNR
# data_type : alpha_bar
# alpha_bar : timestep
# 
# Old alpha_bar and its timestep, and estimated timestep in vs
# 0.99973720  :    0.00231 :    1
# 0.99930942  :    0.00495 :    3
# 0.99818689  :    0.00971 :    8
# 0.99524850  :    0.01780 :   16
# 0.98760682  :    0.03115 :   30
# 0.96806973  :    0.05273 :   51
# 0.92022163  :    0.08693 :   85
# 0.81441766  :    0.13905 :  138
# 0.62541288  :    0.21245 :  211
# 0.38845766  :    0.30332 :  302
# 0.19463222  :    0.40033 :  399
# 0.08420218  :    0.49312 :  492
# 0.03379831  :    0.57754 :  576
# 0.01313375  :    0.65359 :  652
# 0.00503779  :    0.72257 :  721
# 0.00192265  :    0.78582 :  784
# 0.00073236  :    0.84445 :  843
# 0.00027875  :    0.89929 :  898
# 0.00010607  :    0.95099 :  949
# 0.00004036  :    1.00000 :  998
# 
# lr           : 4e-06
# lp           : 0.1
# n_epochs     : 1000
# aa_low       : 0.0001
# aa_low_lambda: 1.0e+07
# beta_schedule: linear
# torch.seed() : 7339084925090840225
# order from param    : None
# order from file     : 1
# order final         : 1
# skip_type from param: None
# skip_type from file : logSNR
# skip_type final     : logSNR
# Epoch       : 000999; loss:477.053360 = 474.983195 + 2.070165
# loss_var    : 963.152429 => 474.983195
# model.lp    : 0.1
# model.out_ch: 20
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999651:   1: 0.999651; 0.000349*1368.170678= 0.477193;  0.477193/0.999651=  0.477359
0.998845:   6: 0.999193; 0.000235*1047.923052= 0.246030;  0.246030/0.998845=  0.246314
0.996701:  13: 0.997854; 0.000552* 790.194487= 0.435857;  0.435857/0.996701=  0.437300
0.991217:  24: 0.994498; 0.001328* 586.771172= 0.779084;  0.779084/0.991217=  0.785987
0.980562:  39: 0.989251; 0.002135* 449.748456= 0.960235;  0.960235/0.980562=  0.979270
0.964274:  55: 0.983389; 0.002576* 361.089927= 0.930225;  0.930225/0.964274=  0.964690
0.941215:  72: 0.976087; 0.003104* 297.800100= 0.924464;  0.924464/0.941215=  0.982203
0.910565:  91: 0.967436; 0.003670* 249.905585= 0.917162;  0.917162/0.910565=  1.007246
0.790193: 148: 0.867805; 0.032205* 166.270851= 5.354715;  5.354715/0.790193=  6.776467
0.569766: 231: 0.721046; 0.071275* 103.808734= 7.398993;  7.398993/0.569766= 12.986028
0.342420: 322: 0.600984; 0.091458*  65.544377= 5.994586;  5.994586/0.342420= 17.506519
0.182365: 407: 0.532577; 0.097622*  41.352863= 4.036937;  4.036937/0.182365= 22.136570
0.091429: 483: 0.501350; 0.097931*  25.754944= 2.522205;  2.522205/0.091429= 27.586577
0.044668: 552: 0.488551; 0.096824*  15.673960= 1.517611;  1.517611/0.044668= 33.975638
0.021599: 613: 0.483539; 0.095778*   9.296436= 0.890391;  0.890391/0.021599= 41.224580
0.010402: 670: 0.481610; 0.095073*   5.338276= 0.507528;  0.507528/0.010402= 48.791033
0.005002: 722: 0.480879; 0.094653*   2.959300= 0.280107;  0.280107/0.005002= 55.997548
0.002404: 770: 0.480586; 0.094427*   1.590508= 0.150187;  0.150187/0.002404= 62.475081
0.001155: 816: 0.480493; 0.094298*   0.826722= 0.077958;  0.077958/0.001155= 67.491382
0.000555: 859: 0.480476; 0.094224*   0.425005= 0.040046;  0.040046/0.000555= 72.155403
