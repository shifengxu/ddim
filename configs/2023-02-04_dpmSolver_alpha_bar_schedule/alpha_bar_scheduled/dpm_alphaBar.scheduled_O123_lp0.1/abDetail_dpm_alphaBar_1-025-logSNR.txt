# Old comments in file ./phase1_ab_original/dpm_alphaBar_1-025-logSNR.txt
# order     : 1
# steps     : 25
# skip_type : logSNR
# data_type : alpha_bar
# alpha_bar : timestep
# 
# Old alpha_bar and its timestep, and estimated timestep in vs
# 0.99978334  :    0.00197 :    0
# 0.99953079  :    0.00369 :    2
# 0.99898380  :    0.00654 :    5
# 0.99780113  :    0.01102 :   10
# 0.99524850  :    0.01780 :   16
# 0.98976260  :    0.02793 :   26
# 0.97808242  :    0.04286 :   41
# 0.95369953  :    0.06460 :   63
# 0.90483028  :    0.09575 :   94
# 0.81441766  :    0.13905 :  138
# 0.66948676  :    0.19611 :  195
# 0.48319510  :    0.26551 :  264
# 0.30145997  :    0.34209 :  341
# 0.16610815  :    0.41944 :  418
# 0.08420218  :    0.49312 :  492
# 0.04071138  :    0.56136 :  560
# 0.01921250  :    0.62410 :  623
# 0.00896071  :    0.68195 :  680
# 0.00415609  :    0.73564 :  734
# 0.00192265  :    0.78582 :  784
# 0.00088837  :    0.83305 :  832
# 0.00041025  :    0.87777 :  876
# 0.00018940  :    0.92032 :  919
# 0.00008743  :    0.96099 :  959
# 0.00004036  :    1.00000 :  998
# 
# lr           : 4e-06
# lp           : 0.1
# n_epochs     : 1000
# aa_low       : 0.0001
# aa_low_lambda: 1.0e+07
# beta_schedule: linear
# torch.seed() : 11977099316997426057
# order from param    : None
# order from file     : 1
# order final         : 1
# skip_type from param: None
# skip_type from file : logSNR
# skip_type final     : logSNR
# Epoch       : 000999; loss:384.841400 = 381.782575 + 3.058825
# loss_var    : 881.904969 => 381.782575
# model.lp    : 0.1
# model.out_ch: 25
# aacum : ts : alpha   ; coef    *weight     =numerator; numerator/aacum   =sub_var
0.999780:   0: 0.999780; 0.000220*1490.761076= 0.327871;  0.327871/0.999780=  0.327943
0.999326:   3: 0.999546; 0.000124*1191.289904= 0.147556;  0.147556/0.999326=  0.147655
0.998322:   8: 0.998995; 0.000226* 952.955775= 0.214915;  0.214915/0.998322=  0.215277
0.996101:  14: 0.997775; 0.000463* 753.004024= 0.348905;  0.348905/0.996101=  0.350271
0.991389:  24: 0.995269; 0.000931* 590.607313= 0.549566;  0.549566/0.991389=  0.554339
0.982442:  36: 0.990976; 0.001610* 465.721586= 0.749920;  0.749920/0.982442=  0.763322
0.968999:  50: 0.986316; 0.001978* 380.297757= 0.752276;  0.752276/0.968999=  0.776343
0.950865:  65: 0.981286; 0.002232* 319.404785= 0.713056;  0.713056/0.950865=  0.749903
0.927778:  81: 0.975720; 0.002479* 273.693943= 0.678353;  0.678353/0.927778=  0.731159
0.899805:  97: 0.969850; 0.002691* 237.819530= 0.639993;  0.639993/0.899805=  0.711257
0.829437: 131: 0.921796; 0.011900* 185.186400= 2.203699;  2.203699/0.829437=  2.656862
0.681487: 190: 0.821626; 0.036107* 129.681640= 4.682398;  4.682398/0.681487=  6.870856
0.493260: 260: 0.723800; 0.053690*  89.422663= 4.801116;  4.801116/0.493260=  9.733443
0.321084: 331: 0.650943; 0.062315*  62.233878= 3.878126;  3.878126/0.321084= 12.078236
0.194850: 399: 0.606852; 0.065243*  43.287044= 2.824188;  2.824188/0.194850= 14.494132
0.113684: 461: 0.583442; 0.065564*  29.785658= 1.952871;  1.952871/0.113684= 17.178091
0.065012: 517: 0.571868; 0.065030*  20.360902= 1.324069;  1.324069/0.065012= 20.366493
0.036820: 569: 0.566356; 0.064377*  13.727316= 0.883717;  0.883717/0.036820= 24.001017
0.020758: 616: 0.563773; 0.063843*   9.031707= 0.576609;  0.576609/0.020758= 27.777558
0.011678: 661: 0.562572; 0.063464*   5.846585= 0.371050;  0.371050/0.011678= 31.773643
0.006563: 703: 0.562016; 0.063215*   3.697810= 0.233757;  0.233757/0.006563= 35.616448
0.003687: 742: 0.561766; 0.063055*   2.293175= 0.144596;  0.144596/0.003687= 39.218128
0.002071: 780: 0.561634; 0.062963*   1.394306= 0.087790;  0.087790/0.002071= 42.395553
0.001163: 815: 0.561580; 0.062905*   0.831406= 0.052299;  0.052299/0.001163= 44.973993
0.000653: 850: 0.561594; 0.062857*   0.491645= 0.030904;  0.030904/0.000653= 47.320652
